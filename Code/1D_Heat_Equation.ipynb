{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\__init__.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     46\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\framework\\ops.py:40\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# pywrap_tensorflow must be imported first to avoid protobuf issues.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# (b/143110113)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: disable=invalid-import-order,g-bad-import-order,unused-import\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# pylint: enable=invalid-import-order,g-bad-import-order,unused-import\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:70\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,line-too-long,undefined-variable\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "# keras.utils.set_random_seed(253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 1 # thermal diffusivity coefficient\n",
    "T_STEP = .1\n",
    "X_STEP = .2\n",
    "SOURCE_TEMP = 100\n",
    "BASE_TEMP = 0\n",
    "LENGTH = 40\n",
    "DURATION = 50\n",
    "\n",
    "\n",
    "def paper_function(x, t)-> float:\n",
    "    return 6 * np.sin((np.pi * x) / LENGTH) * np.exp(-ALPHA * (np.square(np.pi/LENGTH)) * t)\n",
    "\n",
    "def paper_initial_function(x)-> float: # t = 0\n",
    "    return 6 * np.sin((np.pi * x) / LENGTH)\n",
    "\n",
    "\n",
    "# def initial_function(x: float)-> float: # assumes t = 0\n",
    "#     if x == 0 or x == LENGTH: # Boundary Conditions, insulated ends\n",
    "#         return BASE_TEMP\n",
    "    \n",
    "#     temp_diff = SOURCE_TEMP - BASE_TEMP\n",
    "#     if x < .5 * LENGTH:\n",
    "#         return ((x/LENGTH) * 2 * temp_diff) + BASE_TEMP\n",
    "#     else:\n",
    "#         return (temp_diff - (((x - (.5*LENGTH))/LENGTH) * 2 * temp_diff)) + BASE_TEMP\n",
    "    \n",
    "\n",
    "# def heat_equation(T_x1, T_x2, T_x3)-> float:\n",
    "#     # T_x 1, 2, 3 = Temp at x-1, x, and x+1\n",
    "#     # temps are from previous time step\n",
    "    \n",
    "#     return T_x2 + ((ALPHA * T_STEP) * ((T_x3 - (2 * T_x2) + T_x1) / X_STEP**2))\n",
    "\n",
    "    \n",
    "# def heat_equation_boundary(t, x)-> float:\n",
    "#     if x == np.float64(LENGTH) or x == 0: # insulated ends\n",
    "#         return BASE_TEMP\n",
    "#     if t == 0:\n",
    "#         return paper_initial_function(x)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_set = np.arange(0, DURATION, T_STEP, dtype='float64')\n",
    "x_set = np.arange(0, LENGTH, X_STEP, dtype='float64')\n",
    "tx_set = []\n",
    "\n",
    "# look up\n",
    "# np.linspace\n",
    "# np.uniform\n",
    "\n",
    "x_initial = np.arange(X_STEP, LENGTH, X_STEP, dtype='float64')\n",
    "t_initial = np.array([0 for item in x_initial], dtype='float64')\n",
    "x_initial = tf.convert_to_tensor(x_initial, dtype='float64')\n",
    "t_initial = tf.convert_to_tensor(t_initial, dtype='float64')\n",
    "\n",
    "u_initial = np.array([paper_initial_function(x) for x in x_initial], dtype='float64')\n",
    "u_initial = tf.expand_dims(u_initial, axis=1)\n",
    "\n",
    "# print(u_initial)\n",
    "\n",
    "# insulated ends\n",
    "t_boundary = np.array([t for t in t_set] + [t for t in t_set], dtype='float64')\n",
    "x_boundary = np.array([0 for item in t_boundary[0: int(len(t_boundary) / 2)]] + [LENGTH for item in t_boundary[0: int(len(t_boundary) / 2)]], dtype='float64')\n",
    "\n",
    "\n",
    "# t_boundary2 = np.arange(0, DURATION, T_STEP, dtype='float64')\n",
    "# x_boundary2 = np.arange(0, LENGTH + X_STEP, X_STEP, dtype='float64')\n",
    "# u_boundary = []\n",
    "# tx_boundary = []\n",
    "\n",
    "# for item in t_boundary2:\n",
    "#     for item2 in x_boundary2:\n",
    "#         if item == 0:\n",
    "#             tx_boundary.append([item, item2])\n",
    "#             u_boundary.append(np.array([paper_initial_function(item2)]))\n",
    "#         elif item2 == 0 or item2 == LENGTH:\n",
    "#             tx_boundary.append([item, item2])\n",
    "#             u_boundary.append(np.array([BASE_TEMP]))\n",
    "\n",
    "# u_boundary = np.array(u_boundary)\n",
    "# tx_boundary = np.array(tx_boundary)\n",
    "\n",
    "\n",
    "for item in t_set:\n",
    "    if item != 0: \n",
    "        for item2 in x_set:\n",
    "            if item2 != 0: \n",
    "                tx_set.append(np.array([item, item2]))\n",
    "\n",
    "tx_set = np.array(tx_set)\n",
    "\n",
    "# x_b = tf.convert_to_tensor(x_boundary)\n",
    "# t_boundary = tf.squeeze(t_boundary)\n",
    "# x_b = tf.squeeze(x_b)\n",
    "\n",
    "# print(x_b)\n",
    "# print(t_boundary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_network():\n",
    "    layers = []         \n",
    "\n",
    "    layers.append(keras.layers.Dense(16, activation = 'elu', input_shape = (None, 1, 2), dtype='float64'))\n",
    "    layers.append(keras.layers.Dropout(.5))\n",
    "    layers.append(keras.layers.Dense(16, activation = 'elu', dtype='float64'))\n",
    "    layers.append(keras.layers.Dropout(.5))\n",
    "    layers.append(keras.layers.Dense(16, activation = 'elu', dtype='float64'))\n",
    "    layers.append(keras.layers.Dropout(.5))\n",
    "    layers.append(keras.layers.Dense(16, activation = 'elu', dtype='float64'))\n",
    "    layers.append(keras.layers.Dropout(.5))\n",
    "    layers.append(keras.layers.Dense(16, activation = 'elu', dtype='float64'))\n",
    "    layers.append(keras.layers.Dropout(.5))\n",
    "    layers.append(keras.layers.Dense(1, activation = 'linear', dtype='float64'))\n",
    "    \n",
    "    network = keras.Sequential(layers)\n",
    "\n",
    "\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_residual(model: keras.Model, tx_list, alpha)-> tf.Tensor:\n",
    "\n",
    "    t = tx_list[:, 0:1]\n",
    "    x = tx_list[:, 1:2]\n",
    "    t = tf.convert_to_tensor(t, dtype='float64')\n",
    "    x = tf.convert_to_tensor(x, dtype='float64')\n",
    "\n",
    "    t = tf.squeeze(t)\n",
    "    x = tf.squeeze(x)\n",
    "\n",
    "    with tf.GradientTape(persistent= True) as gt:\n",
    "\n",
    "        gt.watch(t)\n",
    "        gt.watch(x)\n",
    "\n",
    "        u = model(tf.stack([t, x], axis=1))\n",
    "        \n",
    "        # print(\"u_x: \" + str(du_dx))\n",
    "        # u = tf.convert_to_tensor(tf.squeeze(u), dtype='float64')\n",
    "\n",
    "        du_dx = gt.gradient(u, x)\n",
    "\n",
    "    du_dt = gt.gradient(u, t)\n",
    "    # print(\"u_t: \" + str(du_dt))\n",
    "    \n",
    "\n",
    "    d2u_dx2 = gt.gradient(du_dx, x) # second derivative of predicted values\n",
    "    # print(\"u_x2: \" + str(d2u_dx2))\n",
    "    del gt\n",
    "\n",
    "    return du_dt - (alpha * d2u_dx2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(model: keras.Model, tx_list, alpha):\n",
    "    \"\"\"\n",
    "    Custom loss function made for neural network. It calculates the residual, and then\n",
    "    adds it to the difference in the boundary outputs and predicted values.\n",
    "    Returns the loss.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    residual = calc_residual(model, tx_list, alpha)  \n",
    "    av_residual = tf.reduce_mean(tf.square(residual)) \n",
    "\n",
    "\n",
    "    # print(tf.stack([x_initial, t_initial], axis=1))\n",
    "    # print(tf.stack([tf.squeeze(x_boundary), tf.squeeze(t_boundary)], axis=1))\n",
    "    \n",
    "\n",
    "    # at beginning, at t = 0, heat should be a gradient from BASE_TEMP at 0, SOURCE_TEMP in the middle, then back down to BASE_TEMP at the end\n",
    "    u_i_pred = model(tf.stack([t_initial, x_initial], axis=1))\n",
    "\n",
    "    u_i_pred = tf.convert_to_tensor(u_i_pred, dtype='float64')\n",
    "\n",
    "    u_i_diff = u_initial - u_i_pred\n",
    "\n",
    "\n",
    "    # print(tf.stack([t_initial, x_initial], axis=1))\n",
    "    # print(u_i_pred)\n",
    "    # print(u_initial)\n",
    "    # print(u_i_diff)\n",
    "\n",
    "    # at any point in time, derivative of x = 0 and x = LENGTH should be 0, as the ends are insulated and heat should not enter or exit\n",
    "    \n",
    "    # with tf.GradientTape() as gt:\n",
    "    \n",
    "    # x_b = tf.convert_to_tensor(x_boundary, dtype='float64')\n",
    "\n",
    "    #     gt.watch(x_b)\n",
    "\n",
    "    u_b_pred = model(tf.stack([tf.squeeze(t_boundary), tf.squeeze(x_boundary)], axis=1))\n",
    "\n",
    "\n",
    "    # du_dx_pred = gt.gradient(u_b_pred, x_b) # derivative should equal 0, insulated ends should not change temp\n",
    "\n",
    "    # print(tf.reduce_mean(tf.square(u_i_diff)))\n",
    "\n",
    "    # print(\"av residual: \" + str(av_residual))\n",
    "    # print(\"Initial condition difference: \" + str(tf.reduce_mean(tf.square(u_i_diff))))\n",
    "    # print(\"Boundary Condition diff: \" + str(tf.reduce_mean(tf.square(u_b_pred))))\n",
    "\n",
    "\n",
    "    return (av_residual) + (tf.reduce_mean(tf.square(u_i_diff))) + (tf.reduce_mean(tf.square(u_b_pred - BASE_TEMP)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient is of the loss function with respect to all the variables in the model\n",
    "def calc_gradient(model: keras.Model, tx_list, alpha):\n",
    "    \"\"\"\n",
    "    Calculates the gradient to apply to the neural network. Returns the\n",
    "    gradient.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    with tf.GradientTape() as gt:\n",
    "        gt.watch(model.trainable_variables) # weights\n",
    "        loss = calc_loss(model, tx_list, alpha)\n",
    "\n",
    "    gradient = gt.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # print(str(gradient))\n",
    "\n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: keras.Model, tx_list, alpha, optimizer: keras.optimizers.Optimizer):\n",
    "    \"\"\"\n",
    "    Executes one training step for the network. Calculates loss and gradient, applies the \n",
    "    gradient, then returns the loss.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    loss, gradient = calc_gradient(model, tx_list, alpha)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "epochs = 5000\n",
    "i = 0\n",
    "network = make_network()\n",
    "# network.load_weights(\"1D_heat_equation_network.h5\")\n",
    "\n",
    "while i < epochs:\n",
    "    \n",
    "    loss = 0\n",
    "    loss = train(network, tx_set, ALPHA, keras.optimizers.Adam())\n",
    "\n",
    "\n",
    "    print(\"Epoch: \" + str(i))\n",
    "    print(\"Loss: \" + str(loss))\n",
    "    print(\"\\n\")\n",
    "    # save every epoch so i can jump ship at any time and still have a network to show for it\n",
    "    if i % 10 == 0:\n",
    "        network.save(\"1D_heat_equation_network.h5\")\n",
    "    i += 1\n",
    "\n",
    "network.save(\"1D_heat_equation_network.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "actual_temps = []\n",
    "\n",
    "x_full = np.arange(0, LENGTH + X_STEP, X_STEP, dtype='float32')\n",
    "\n",
    "for number in range(len(t_set)):\n",
    "\n",
    "    actual_temps.append(np.array([paper_function(x, t_set[number]) for x in x_full]))\n",
    "\n",
    "    # if not number == 0:\n",
    "    #     # normally I love list comprehensions, but WOW this is garbage\n",
    "    #     actual_temps.append(np.array([heat_equation_boundary(t_set[number], x_full[i]) if i == 0 or i == len(x_full) - 1 else heat_equation(actual_temps[number-1][i-1], actual_temps[number-1][i], actual_temps[number-1][i+1]) for i in range(len(x_full))]))\n",
    "\n",
    "    # else: \n",
    "    #     actual_temps.append(np.array([heat_equation_boundary(t_set[number], x) for x in x_full]))\n",
    "    \n",
    "\n",
    "actual_temps = tf.convert_to_tensor(actual_temps, dtype='float32')\n",
    "\n",
    "if True:\n",
    "    network = make_network()\n",
    "    network.load_weights(\"1D_heat_equation_network.h5\")\n",
    "    predictions = []\n",
    "\n",
    "    x = tf.convert_to_tensor(x_full, dtype='float32')\n",
    "\n",
    "    for number in t_set:\n",
    "\n",
    "        time = np.array([number for item in x])\n",
    "\n",
    "        time = tf.convert_to_tensor(time, dtype='float32')\n",
    "        \n",
    "        y = network(tf.stack([time, x], axis=1))\n",
    "        predictions.append(y)\n",
    "\n",
    "# return 6 * np.sin((np.pi * x) / LENGTH) * np.exp(-ALPHA * (np.pi/LENGTH)**2 * t)\n",
    "\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# plt.xlabel(\"Position\")\n",
    "# plt.ylabel(\"Temp\")\n",
    "# plt.axis((-.2, 6, 0, 101))\n",
    "\n",
    "# for item in predictions:\n",
    "#     color = np.where(item < 20, 'k', np.where(item < 40, 'b', np.where(item < 60, 'g', np.where(item < 80, 'y', 'r'))))\n",
    "#     plt.scatter(x, item, s=5, c=np.squeeze(color), linewidth=0)\n",
    "    \n",
    "#     display(plt.scatter(x, item, s=5, c=np.squeeze(color), linewidth=0))\n",
    "#     clear_output(wait=True)\n",
    "#     plt.pause(.1)\n",
    "\n",
    "figure = plt.figure()\n",
    "sub1 = figure.add_subplot(111)\n",
    "sub1.set_xlabel(\"Position\")\n",
    "sub1.set_ylabel(\"Temp\")\n",
    "sub1.axis((-1, LENGTH + 2, -5, 8))\n",
    "\n",
    "# line1, = sub1.plot(x, predictions[0], \"r--\")\n",
    "\n",
    "\n",
    "\n",
    "for number in range(len(actual_temps)):\n",
    "   \n",
    "    color1 = np.where(actual_temps[number] < 1, 'k', np.where(actual_temps[number] < 2, 'b', np.where(actual_temps[number]< 3, 'g', np.where(actual_temps[number] < 4, 'y', 'r'))))    \n",
    "    sub1.plot(x_full, actual_temps[number], 'b-', linewidth=0.5, zorder=0)\n",
    "    sub1.scatter(x_full, actual_temps[number], s=5, c=np.squeeze(color1), zorder=1)\n",
    "\n",
    "    if True:\n",
    "        color2 = np.where(predictions[number] < 1, 'k', np.where(predictions[number] < 2, 'b', np.where(predictions[number]< 3, 'g', np.where(predictions[number] < 4, 'y', 'r'))))    \n",
    "        sub1.plot(x_full, predictions[number], 'r--', linewidth=0.5, zorder=2)\n",
    "        sub1.scatter(x_full, predictions[number], s=5, c=np.squeeze(color2), zorder=3)\n",
    "\n",
    "    sub1.axis((-1, LENGTH + 2, -5, 8))\n",
    "    display(figure)\n",
    "    clear_output(wait=True)\n",
    "    plt.pause(.1)\n",
    "    sub1.clear()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network = make_network()\n",
    "network.load_weights(\"1D_heat_equation_network.h5\")\n",
    "\n",
    "t_list = np.array([.6, 2.4, 4.2, 7.7, 9.1, 10.31, 17.9], dtype='float64')\n",
    "x_list = np.array([.5, 4.83, 10.3, 13.43, 20.7, 25.6, 30.2, 39.9], dtype='float64')\n",
    "\n",
    "x = tf.convert_to_tensor(x_list)\n",
    "\n",
    "for item in t_list:\n",
    "    time = np.array([item for x in x_list])\n",
    "    t = tf.convert_to_tensor(time)\n",
    "\n",
    "    with tf.GradientTape(persistent= True) as gt:\n",
    "        gt.watch(x)\n",
    "        gt.watch(t)\n",
    "\n",
    "        u = network(tf.stack([t, x], axis=1))\n",
    "\n",
    "        u_x = gt.gradient(u, x)\n",
    "    u_t = gt.gradient(u, t)\n",
    "    u_xx = gt.gradient(u_x, x)\n",
    "\n",
    "    del(gt)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"time = \" + str(item) + \": \")\n",
    "    print(\"u: \" + str(u))\n",
    "    print(\"uxx: \" + str(u_xx))\n",
    "    print(\"ut: \" + str(u_t))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
