{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "keras.utils.set_random_seed(253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yajuns_function(t: np.float32):\n",
    "    return(-1 * np.power(np.e, t)) + np.power(np.e, 2*t) \n",
    "\n",
    "t_test = np.arange(0, 5, .01, dtype='float32')\n",
    "y_test = np.array([yajuns_function(t) for t in t_test], dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual is how accurate the output is with respect to target equation\n",
    "def calc_residual(model: keras.Model, alpha: tf.Variable, beta: tf.Variable, t_vals)-> tf.Tensor:\n",
    "    \n",
    "    t_vals_tensor = tf.convert_to_tensor(t_vals, dtype='float32')\n",
    "\n",
    "\n",
    "    with tf.GradientTape(persistent= True) as gt:\n",
    "\n",
    "        gt.watch(t_vals_tensor)\n",
    "\n",
    "        y = model(t_vals_tensor)\n",
    "\n",
    "        # print(t_vals_tensor)\n",
    "        \n",
    "        # predicted values\n",
    "        y_prime = gt.gradient(y, t_vals_tensor)\n",
    "        \n",
    "        y = tf.convert_to_tensor(tf.squeeze(y), dtype='float32')\n",
    "\n",
    "    y_dprime = gt.gradient(y_prime, t_vals_tensor) # second derivative of predicted values\n",
    "    del gt\n",
    "\n",
    "    # print(\"\\n\")\n",
    "    # print(\"t: \"+ str(type(t_vals_tensor)))\n",
    "    # print(\"y: \" + str(y))\n",
    "    # print(\"dt: \" + str(tf.reduce_mean(y_prime)))\n",
    "    # print(\"d2t: \" + str(y_dprime))\n",
    "    # print(\"residual: \" + str(tf.reduce_mean(tf.add(y_dprime, tf.add(tf.scalar_mul(alpha, y_prime), tf.scalar_mul(beta, y))))))\n",
    "    # print(\"\\n\")\n",
    "    \n",
    "    return tf.add(y_dprime, tf.add(tf.scalar_mul(alpha, y_prime), tf.scalar_mul(beta, y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate how accurate the model is with respect to the actual answers\n",
    "def calc_loss(model: keras.Model, alpha: tf.Variable, beta: tf.Variable, t_vals):\n",
    "    \"\"\"\n",
    "    Custom loss function made for neural network. It calculates the residual, and then\n",
    "    adds it to the difference in the boundary outputs and predicted values.\n",
    "    Returns the loss.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # need residual for loss, it will tell us how accurate vals are with respect to itself\n",
    "    residual = calc_residual(model, alpha, beta, t_vals)  \n",
    "    av_residual = tf.reduce_mean(tf.square(residual))\n",
    "\n",
    "    t = tf.convert_to_tensor(np.array([0.0], dtype = 'float32'))\n",
    "    with tf.GradientTape() as gt:\n",
    "        gt.watch(t)\n",
    "        y = model(t) # y(0) should equal 0\n",
    "        \n",
    "    dy_dt = gt.gradient(y, t) # y'(0) should equal 1    \n",
    "\n",
    "    \n",
    "    # print(\"\\n\")\n",
    "    # print(\"average residual: \" + str(av_residual))\n",
    "    # print(\"y: \" + str(y))\n",
    "    # print(\"dy_dt: \" + str(dy_dt))\n",
    "    # print(\"dy_dt -1: \" + str(dy_dt - 1))\n",
    "    # print(\"returns: \" + str(tf.add(av_residual, tf.add(tf.pow(y, 2), tf.pow(tf.subtract(dy_dt, 1), 2)))))\n",
    "    # print(\"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    return tf.add(av_residual, tf.add(tf.square(y), tf.square(tf.subtract(dy_dt, 1)))) # residual should equal 0, y should equal 0, and dt should equal 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient is of the loss function with respect to all the variables in the model\n",
    "def calc_gradient(model: keras.Model, alpha: tf.Variable, beta: tf.Variable, t_vals):\n",
    "    \"\"\"\n",
    "    Calculates the gradient to apply to the neural network. Returns the\n",
    "    gradient.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    with tf.GradientTape() as gt:\n",
    "        gt.watch(model.trainable_variables) # weights\n",
    "        loss = calc_loss(model, alpha, beta, t_vals)\n",
    "\n",
    "    gradient = gt.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # print(str(gradient))\n",
    "\n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: keras.Model, alpha: tf.Variable, beta: tf.Variable, t_vals, optimizer: keras.optimizers.Optimizer):\n",
    "    \"\"\"\n",
    "    Executes one training step for the network. Calculates loss and gradient, applies the \n",
    "    gradient, then returns the loss.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    loss, gradient = calc_gradient(model, alpha, beta, t_vals)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make layers of the model\n",
    "# most accurate 2 period wave is\n",
    "# 20, 15, 10, 5 with .5 dropout in between\n",
    "\n",
    "def make_network() -> tuple:\n",
    "    layers = []         \n",
    "\n",
    "    layers.append(keras.layers.Dense(16, activation = 'elu', input_shape = (1,)))\n",
    "    layers.append(keras.layers.Dense(16, activation = 'elu'))\n",
    "    layers.append(keras.layers.Dense(16, activation = 'elu'))\n",
    "\n",
    "    output_layer = keras.layers.Dense(1, activation = 'linear', name = \"output\")\n",
    "    layers.append(output_layer)\n",
    "\n",
    "    output_layer.add_weight(shape = (), dtype='float32', trainable=True, name = 'alpha')\n",
    "    output_layer.add_weight(shape = (), dtype='float32', trainable=True, name = 'beta')\n",
    "    \n",
    "    network = keras.Sequential(layers)\n",
    "\n",
    "    #print(output_layer.trainable_weights)\n",
    "\n",
    "    return (network, output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up batches\n",
    "batch_size = 1000000\n",
    "\n",
    "t_batches = []\n",
    "\n",
    "i = 0\n",
    "while i < (len(t_test) / batch_size):\n",
    "    t_batches.append(np.array(t_test[i * batch_size : (i * batch_size) + batch_size], dtype='float32'))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 0\n",
      "Loss: tf.Tensor([[1.1323166]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.46383363>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1994174>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 1\n",
      "Loss: tf.Tensor([[0.9472865]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.46483362>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1984173>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 2\n",
      "Loss: tf.Tensor([[0.7336477]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.46583357>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1974174>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 3\n",
      "Loss: tf.Tensor([[0.58065057]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.4668335>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1964175>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 4\n",
      "Loss: tf.Tensor([[0.4684463]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.46783242>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1954175>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 5\n",
      "Loss: tf.Tensor([[0.37101465]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.4668325>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1944176>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 6\n",
      "Loss: tf.Tensor([[0.3168502]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.46583253>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1934178>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 7\n",
      "Loss: tf.Tensor([[0.2921272]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.46483257>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1944172>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 8\n",
      "Loss: tf.Tensor([[0.28254348]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.4638326>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1934177>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 9\n",
      "Loss: tf.Tensor([[0.28975466]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.46283263>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.194417>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 10\n",
      "Loss: tf.Tensor([[0.29501635]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.46183264>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1934173>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 11\n",
      "Loss: tf.Tensor([[0.29985157]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.4608327>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1944165>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 12\n",
      "Loss: tf.Tensor([[0.29494554]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.4598327>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1934168>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 13\n",
      "Loss: tf.Tensor([[0.29823124]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.45883274>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1944157>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 14\n",
      "Loss: tf.Tensor([[0.29123616]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.45783275>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1934159>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 15\n",
      "Loss: tf.Tensor([[0.29833096]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.4568328>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1944145>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 16\n",
      "Loss: tf.Tensor([[0.29099697]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.4558328>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1934147>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 17\n",
      "Loss: tf.Tensor([[0.30068436]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.45483285>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1944119>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 18\n",
      "Loss: tf.Tensor([[0.28925803]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.45383286>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1934121>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 19\n",
      "Loss: tf.Tensor([[0.29674864]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.4528329>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1924351>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 20\n",
      "Loss: tf.Tensor([[0.29404566]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.45183292>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1914353>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 21\n",
      "Loss: tf.Tensor([[0.28908166]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.45083296>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.1904379>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n",
      "Epoch: 22\n",
      "Loss: tf.Tensor([[0.29098767]], shape=(1, 1), dtype=float32)\n",
      "Alpha: <tf.Variable 'alpha:0' shape=() dtype=float32, numpy=-0.44983298>\n",
      "Beta: <tf.Variable 'beta:0' shape=() dtype=float32, numpy=1.189438>\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "tf.Tensor(\n",
      "[0.         0.01       0.02       0.03       0.04       0.05\n",
      " 0.06       0.07       0.08       0.09       0.09999999 0.11\n",
      " 0.12       0.13       0.14       0.14999999 0.16       0.17\n",
      " 0.17999999 0.19       0.19999999 0.21       0.22       0.22999999\n",
      " 0.24       0.25       0.26       0.26999998 0.28       0.29\n",
      " 0.29999998 0.31       0.32       0.32999998 0.34       0.35\n",
      " 0.35999998 0.37       0.38       0.39       0.39999998 0.41\n",
      " 0.42       0.42999998 0.44       0.45       0.45999998 0.47\n",
      " 0.48       0.48999998 0.5        0.51       0.52       0.53\n",
      " 0.53999996 0.55       0.56       0.57       0.58       0.59\n",
      " 0.59999996 0.61       0.62       0.63       0.64       0.65\n",
      " 0.65999997 0.66999996 0.68       0.69       0.7        0.71\n",
      " 0.71999997 0.72999996 0.74       0.75       0.76       0.77\n",
      " 0.78       0.78999996 0.79999995 0.81       0.82       0.83\n",
      " 0.84       0.84999996 0.85999995 0.87       0.88       0.89\n",
      " 0.9        0.90999997 0.91999996 0.93       0.94       0.95\n",
      " 0.96       0.96999997 0.97999996 0.98999995 1.         1.01\n",
      " 1.02       1.03       1.04       1.05       1.06       1.0699999\n",
      " 1.0799999  1.09       1.1        1.11       1.12       1.13\n",
      " 1.14       1.15       1.16       1.17       1.18       1.1899999\n",
      " 1.1999999  1.2099999  1.22       1.23       1.24       1.25\n",
      " 1.26       1.27       1.28       1.29       1.3        1.31\n",
      " 1.3199999  1.3299999  1.3399999  1.35       1.36       1.37\n",
      " 1.38       1.39       1.4        1.41       1.42       1.43\n",
      " 1.4399999  1.4499999  1.4599999  1.4699999  1.48       1.49\n",
      " 1.5        1.51       1.52       1.53       1.54       1.55\n",
      " 1.56       1.5699999  1.5799999  1.5899999  1.5999999  1.61\n",
      " 1.62       1.63       1.64       1.65       1.66       1.67\n",
      " 1.68       1.6899999  1.6999999  1.7099999  1.7199999  1.73\n",
      " 1.74       1.75       1.76       1.77       1.78       1.79\n",
      " 1.8        1.81       1.8199999  1.8299999  1.8399999  1.8499999\n",
      " 1.86       1.87       1.88       1.89       1.9        1.91\n",
      " 1.92       1.93       1.9399999  1.9499999  1.9599999  1.9699999\n",
      " 1.9799999  1.99       2.         2.01       2.02       2.03\n",
      " 2.04       2.05       2.06       2.07       2.08       2.09\n",
      " 2.1        2.11       2.12       2.1299999  2.1399999  2.1499999\n",
      " 2.1599998  2.1699998  2.18       2.19       2.2        2.21\n",
      " 2.22       2.23       2.24       2.25       2.26       2.27\n",
      " 2.28       2.29       2.3        2.31       2.32       2.33\n",
      " 2.34       2.35       2.36       2.37       2.3799999  2.3899999\n",
      " 2.3999999  2.4099998  2.4199998  2.4299998  2.44       2.45\n",
      " 2.46       2.47       2.48       2.49       2.5        2.51\n",
      " 2.52       2.53       2.54       2.55       2.56       2.57\n",
      " 2.58       2.59       2.6        2.61       2.62       2.6299999\n",
      " 2.6399999  2.6499999  2.6599998  2.6699998  2.6799998  2.69\n",
      " 2.7        2.71       2.72       2.73       2.74       2.75\n",
      " 2.76       2.77       2.78       2.79       2.8        2.81\n",
      " 2.82       2.83       2.84       2.85       2.86       2.87\n",
      " 2.8799999  2.8899999  2.8999999  2.9099998  2.9199998  2.9299998\n",
      " 2.9399998  2.95       2.96       2.97       2.98       2.99\n",
      " 3.         3.01       3.02       3.03       3.04       3.05\n",
      " 3.06       3.07       3.08       3.09       3.1        3.11\n",
      " 3.12       3.1299999  3.1399999  3.1499999  3.1599998  3.1699998\n",
      " 3.1799998  3.1899998  3.1999998  3.21       3.22       3.23\n",
      " 3.24       3.25       3.26       3.27       3.28       3.29\n",
      " 3.3        3.31       3.32       3.33       3.34       3.35\n",
      " 3.36       3.37       3.3799999  3.3899999  3.3999999  3.4099998\n",
      " 3.4199998  3.4299998  3.4399998  3.4499998  3.46       3.47\n",
      " 3.48       3.49       3.5        3.51       3.52       3.53\n",
      " 3.54       3.55       3.56       3.57       3.58       3.59\n",
      " 3.6        3.61       3.62       3.6299999  3.6399999  3.6499999\n",
      " 3.6599998  3.6699998  3.6799998  3.6899998  3.6999998  3.7099998\n",
      " 3.72       3.73       3.74       3.75       3.76       3.77\n",
      " 3.78       3.79       3.8        3.81       3.82       3.83\n",
      " 3.84       3.85       3.86       3.87       3.8799999  3.8899999\n",
      " 3.8999999  3.9099998  3.9199998  3.9299998  3.9399998  3.9499998\n",
      " 3.9599998  3.97       3.98       3.99       4.         4.0099998\n",
      " 4.02       4.0299997  4.04       4.0499997  4.06       4.0699997\n",
      " 4.08       4.0899997  4.1        4.11       4.12       4.13\n",
      " 4.14       4.15       4.16       4.17       4.18       4.19\n",
      " 4.2        4.21       4.22       4.23       4.24       4.25\n",
      " 4.2599998  4.27       4.2799997  4.29       4.2999997  4.31\n",
      " 4.3199997  4.33       4.3399997  4.35       4.36       4.37\n",
      " 4.38       4.39       4.4        4.41       4.42       4.43\n",
      " 4.44       4.45       4.46       4.47       4.48       4.49\n",
      " 4.5        4.5099998  4.52       4.5299997  4.54       4.5499997\n",
      " 4.56       4.5699997  4.58       4.5899997  4.6        4.61\n",
      " 4.62       4.63       4.64       4.65       4.66       4.67\n",
      " 4.68       4.69       4.7        4.71       4.72       4.73\n",
      " 4.74       4.75       4.7599998  4.77       4.7799997  4.79\n",
      " 4.7999997  4.81       4.8199997  4.83       4.8399997  4.85\n",
      " 4.8599997  4.87       4.88       4.89       4.9        4.91\n",
      " 4.92       4.93       4.94       4.95       4.96       4.97\n",
      " 4.98       4.99      ], shape=(500,), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11308\\1058354662.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# train each batch per epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mj\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11308\\4044762651.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, alpha, beta, t_vals, optimizer)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11308\\3110141773.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, alpha, beta, t_vals)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mgt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11308\\4055482709.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, alpha, beta, t_vals)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mgt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# y(0) should equal 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mdy_dt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# y'(0) should equal 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# print(\"\\n\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1062\u001b[0m               output_gradients))\n\u001b[0;32m   1063\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[0;32m   1064\u001b[0m                           for x in output_gradients]\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1066\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     raise ValueError(\n\u001b[0;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gradient_tape/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1708\u001b[0m   \u001b[0mt_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"transpose_b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1709\u001b[0m   \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m   \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1711\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1712\u001b[1;33m     \u001b[0mgrad_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1713\u001b[0m     \u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1714\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1715\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6173\u001b[0m         transpose_b)\n\u001b[0;32m   6174\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6175\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6176\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6177\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6178\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6180\u001b[0m       return mat_mul_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "epochs = 1000\n",
    "i = 0\n",
    "network, output_layer = make_network()\n",
    "alpha = output_layer.trainable_weights[0]\n",
    "beta = output_layer.trainable_weights[1]\n",
    "\n",
    "while i < epochs:\n",
    "\n",
    "    loss = 0\n",
    "    j = 0\n",
    "\n",
    "    while j < len(t_batches): # train each batch per epoch\n",
    "        loss += train(network, alpha, beta, t_batches[j], keras.optimizers.Adam())\n",
    "        j += 1\n",
    "\n",
    "\n",
    "    print(\"Epoch: \" + str(i))\n",
    "    print(\"Loss: \" + str(loss / j))\n",
    "    print(\"Alpha: \" + str(alpha))\n",
    "    print(\"Beta: \" + str(beta))\n",
    "    print(\"\\n\")\n",
    "    # save every epoch so i can jump ship at any time and still have a network to show for it\n",
    "    network.save(\"alpha_beta_problem.weights.h5\")\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m display(figure)\n\u001b[0;32m     39\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 40\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpause\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\matplotlib\\pyplot.py:665\u001b[0m, in \u001b[0;36mpause\u001b[1;34m(interval)\u001b[0m\n\u001b[0;32m    663\u001b[0m     canvas\u001b[38;5;241m.\u001b[39mstart_event_loop(interval)\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 665\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(interval)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "network, output_layer = make_network()\n",
    "network.load_weights(\"alpha_beta_problem.weights.h5\", skip_mismatch=True)\n",
    "x = np.arange(0, 5, .01, dtype='float32')\n",
    "t = np.arange(0, 5, .01, dtype='float32')\n",
    "# y = list(map(lambda a: a*10,y)) # magnification :(\n",
    "\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for number in t:\n",
    "    prediction = network.predict(tf.scalar_mul(number, x), verbose = 0)\n",
    "    y = np.squeeze(prediction)\n",
    "\n",
    "    # y = [2000 * number * np.sin(item) for item in x]\n",
    "\n",
    "    \n",
    "    predictions.append(y)\n",
    "\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "figure = plt.figure()\n",
    "sub1 = figure.add_subplot(111)\n",
    "\n",
    "line1, = sub1.plot(t_test, y_test, 'b-')\n",
    "line2, = sub1.plot(x, predictions[0], \"r--\")\n",
    "\n",
    "# for item in predictions:\n",
    "\n",
    "#     line2.set_ydata(item)\n",
    "\n",
    "#     figure.canvas.draw()\n",
    "\n",
    "for item in predictions:\n",
    "    line2.set_ydata(item)\n",
    "    display(figure)\n",
    "    clear_output(wait=True)\n",
    "    plt.pause(.2)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
